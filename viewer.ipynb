{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d0c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path\n",
    "from os.path import abspath\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "import json\n",
    "import h5py\n",
    "import random\n",
    "import yaml\n",
    "from addict import Dict\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea04f686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PFbeta(labels, predictions, beta, eps=1e-5):\n",
    "    # eps is a small error term added for numerical stability\n",
    "    y_true_count = 0\n",
    "    ctp = 0\n",
    "    cfp = 0\n",
    "\n",
    "    for idx in range(len(labels)):\n",
    "        prediction = min(max(predictions[idx], 0), 1)\n",
    "        if (labels[idx]):\n",
    "            y_true_count += 1\n",
    "            ctp += prediction\n",
    "        else:\n",
    "            cfp += prediction\n",
    "\n",
    "    beta_squared = beta * beta\n",
    "    c_precision = (ctp + eps) / (ctp + cfp + eps)\n",
    "    c_recall = (ctp + eps) / (y_true_count + eps)\n",
    "    if (c_precision > 0 and c_recall > 0):\n",
    "        result = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall)\n",
    "        return result\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "def MovingAvg(v, size):\n",
    "    avg = np.empty_like(v)\n",
    "    avg[:size] = np.mean(v[:2*size])\n",
    "    avg[len(v)-2*size+1:len(v)] = np.mean(v[len(v)-2*size+1:len(v)])\n",
    "    for i in range(size, len(v) - size):\n",
    "        avg[i] = np.mean(v[i-size:i+size])\n",
    "    return avg\n",
    "\n",
    "def ConfusionMatrix(p, gt):\n",
    "    p = np.round(np.array(p))\n",
    "    gt = np.array(gt)\n",
    "    tp = np.sum((p == 1) & (gt == 1)).astype(int)\n",
    "    fp = np.sum((p == 1) & (gt == 0)).astype(int)\n",
    "    tn = np.sum((p == 0) & (gt == 0)).astype(int)\n",
    "    fn = np.sum((p == 0) & (gt == 1)).astype(int)\n",
    "    return tp, fp, tn ,fn\n",
    "\n",
    "def MetricsFromCM(tp, fp, tn, fn, epsilon=1.0e-6):\n",
    "    tpr = (tp + epsilon)/(tp + fn + epsilon)\n",
    "    fpr = (fp + epsilon)/(fp + tn + epsilon)\n",
    "    precision = (tp + epsilon)/(tp + fp + epsilon)\n",
    "    tnr = (tn + epsilon)/(fp + tn + epsilon)\n",
    "    return tpr, fpr, precision, tnr\n",
    "        \n",
    "def CreateTensorStats(df, labels):\n",
    "    plabel = labels[0]\n",
    "    tlabel = labels[1]\n",
    "    flabel = labels[2]\n",
    "    true_pos, false_pos, true_neg, false_neg = [], [], [], []\n",
    "    true_pr, false_pr, precision, true_nr = [], [], [], []\n",
    "    f1scores = []\n",
    "    for i in range(len(df.index)):\n",
    "        preds = eval(df.loc[i, plabel]).cpu().tolist()\n",
    "        truths = eval(df.loc[i, tlabel]).cpu().tolist()\n",
    "        f1 = eval(df.loc[i, flabel]).cpu().item()\n",
    "        df.loc[i, plabel] = preds\n",
    "        df.loc[i, tlabel] = truths\n",
    "        f1scores.append(f1)\n",
    "        tp, fp, tn, fn = ConfusionMatrix(preds, truths)\n",
    "        tpr, fpr, prec, tnr = MetricsFromCM(tp, fp, tn, fn)\n",
    "        true_pos.append(tp)\n",
    "        false_pos.append(fp)\n",
    "        true_neg.append(tn)\n",
    "        false_neg.append(fn)\n",
    "        true_pr.append(tpr)\n",
    "        false_pr.append(fpr)\n",
    "        precision.append(prec)\n",
    "        true_nr.append(tnr)\n",
    "    df[flabel] = f1scores\n",
    "    df[\"tp\"] = true_pos\n",
    "    df[\"fp\"] = false_pos\n",
    "    df[\"tn\"] = true_neg\n",
    "    df[\"fn\"] = false_neg\n",
    "    df[\"tpr\"] = true_pr\n",
    "    df[\"fpr\"] = false_pr\n",
    "    df[\"precision\"] = precision\n",
    "    df[\"tnr\"] = true_nr\n",
    "    return\n",
    "\n",
    "def CreateStats(df, labels, idx):\n",
    "    plabel = labels[0]\n",
    "    tlabel = labels[1]\n",
    "    true_pos, false_pos, true_neg, false_neg = [], [], [], []\n",
    "    true_pr, false_pr, precision, true_nr = [], [], [], []\n",
    "    for i in range(len(df.index)):\n",
    "        preds = ast.literal_eval(df.loc[i, plabel])\n",
    "        truths = ast.literal_eval(df.loc[i, tlabel])\n",
    "        tp, fp, tn, fn = ConfusionMatrix(np.asarray(preds)[:, idx],\n",
    "                                         np.asarray(truths)[:, idx])\n",
    "        tpr, fpr, prec, tnr = MetricsFromCM(tp, fp, tn, fn)\n",
    "        true_pos.append(tp)\n",
    "        false_pos.append(fp)\n",
    "        true_neg.append(tn)\n",
    "        false_neg.append(fn)\n",
    "        true_pr.append(tpr)\n",
    "        false_pr.append(fpr)\n",
    "        precision.append(prec)\n",
    "        true_nr.append(tnr)\n",
    "    df[\"tp\"] = true_pos\n",
    "    df[\"fp\"] = false_pos\n",
    "    df[\"tn\"] = true_neg\n",
    "    df[\"fn\"] = false_neg\n",
    "    df[\"tpr\"] = true_pr\n",
    "    df[\"fpr\"] = false_pr\n",
    "    df[\"precision\"] = precision\n",
    "    df[\"tnr\"] = true_nr\n",
    "    return\n",
    "\n",
    "def StrListColumnToArray(df: pd.DataFrame, src_column: str):\n",
    "    return np.array(df.loc[:, src_column].apply(ast.literal_eval).to_list())\n",
    "\n",
    "def ArrayToColumns(df: pd.DataFrame, arr: np.ndarray, new_columns: list):\n",
    "    assert arr.shape[1] == len(new_columns)\n",
    "    return pd.concat((df, pd.DataFrame(arr, columns=new_columns)), axis=1)\n",
    "\n",
    "def BalancedF1Score(preds, gt, n=None):\n",
    "    true_ids = np.arange(len(gt))[gt == 1].astype(int)\n",
    "    false_ids = np.arange(len(gt))[gt == 0].astype(int)\n",
    "    if n == None:\n",
    "        n = len(true_ids)\n",
    "    sel_true_ids = np.random.choice(true_ids, n).astype(int)\n",
    "    sel_false_ids = np.random.choice(false_ids, n).astype(int)\n",
    "    sel_gt = np.concatenate((gt[sel_true_ids], gt[sel_false_ids]))\n",
    "    sel_preds = np.concatenate((preds[sel_true_ids], preds[sel_false_ids]))\n",
    "    return PFbeta(sel_gt, sel_preds, beta=1.)\n",
    "\n",
    "def ConvertTensors(df, labels):\n",
    "    for label in labels:\n",
    "        for i in range(len(df.index)):\n",
    "            val = eval(df.loc[i, label]).cpu()\n",
    "            if len(val) > 1:\n",
    "                df.loc[i,label] = val.tolist()\n",
    "            else:\n",
    "                df.loc[i,label] = val.item()\n",
    "\n",
    "def Dashboard(report: pd.DataFrame, window_size: int=100):\n",
    "    loss_ma = MovingAvg(report.loss, window_size)\n",
    "    bacc_ma = MovingAvg(report.block_accuracy, window_size)\n",
    "    lr_ma = MovingAvg(report.learning_rate, window_size)\n",
    "    eacc = report.groupby(\"epoch\").mean(numeric_only=True)[[\"block_accuracy\", \"epoch_accuracy\"]]\n",
    "    recall, fpr, precision, tnr = MetricsFromCM(report)\n",
    "    report[\"recall\"] = recall\n",
    "    report[\"fpr\"] = fpr\n",
    "    report[\"precision\"] = precision\n",
    "    report[\"tnr\"] = tnr\n",
    "    f1_scores = report.groupby(\"epoch\").mean(numeric_only=True)[\"f1_scores\"]\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(12,8))\n",
    "    fig.suptitle(\"Training Log\")\n",
    "    # plot loss\n",
    "    axs[0, 0].set_title(\"loss\")\n",
    "    axs[0, 0].set_xlabel(\"block\")\n",
    "    axs[0, 0].set_ylabel(\"loss\")\n",
    "    axs[0, 0].scatter(report.index, report.loss, s=0.3, c=\"red\", label=\"value\")\n",
    "    axs[0, 0].plot(report.index, loss_ma, \"-b\", label=\"moving avg\")\n",
    "    axs[0, 0].legend()\n",
    "    # plot block acc\n",
    "    axs[0, 1].set_title(\"block accuracy\")\n",
    "    axs[0, 1].set_xlabel(\"block\")\n",
    "    axs[0, 1].set_ylabel(\"accuracy\")\n",
    "    axs[0, 1].scatter(report.index, report.block_accuracy, s=0.3, c=\"red\")\n",
    "    axs[0, 1].plot(report.index, bacc_ma, \"-b\")\n",
    "    # plot epoch acc vs mean block_acc\n",
    "    axs[0, 2].set_title(\"epoch accuracy\")\n",
    "    axs[0, 2].set_xlabel(\"epoch\")\n",
    "    axs[0, 2].set_ylabel(\"accuracy\")\n",
    "    axs[0, 2].plot(eacc.index, eacc.epoch_accuracy, \"-r\",\n",
    "                   linewidth=0.3, label=\"eval\")\n",
    "    axs[0, 2].plot(eacc.index, eacc.block_accuracy, \"-g\",\n",
    "                   linewidth=0.3, label=\"train\")\n",
    "    axs[0, 2].legend()\n",
    "    # plot epoch PFbeta\n",
    "    axs[1, 0].set_title(\"F1 Score\")\n",
    "    axs[1, 0].set_xlabel(\"epoch\")\n",
    "    axs[1, 0].set_ylabel(\"accuracy\")\n",
    "    axs[1, 0].plot(f1_scores.index, f1_scores, \"-r\",\n",
    "                   linewidth=0.3)\n",
    "    # plot Recall, Precision and TNR\n",
    "    axs[1, 1].set_title(\"AUC\")\n",
    "    axs[1, 1].set_xlabel(\"1 - Specificity (FPR)\")\n",
    "    axs[1, 1].set_ylabel(\"Sensitivity (TPR)\")\n",
    "    axs[1, 1].plot(fpr, recall, \"-r\",\n",
    "                   linewidth=1.)\n",
    "    axs[1, 1].plot([0., 1.], [0., 1.], \"-k\", linewidth=0.5)\n",
    "    # plot learning rate\n",
    "    axs[1, 2].set_title(\"Learning Rate\")\n",
    "    axs[1, 2].set_xlabel(\"block\")\n",
    "    axs[1, 2].set_ylabel(\"accuracy\")\n",
    "    axs[1, 2].scatter(report.index, report.learning_rate, s=0.3, c=\"red\")\n",
    "    axs[1, 2].plot(report.index, lr_ma, \"-b\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def Dashboard2(train_report: pd.DataFrame, eval_report, window_size: int=100):\n",
    "    CreateStats(train_report, [\"predictions\", \"truths\"], 0)\n",
    "    CreateStats(eval_report, [\"predictions\", \"truths\"], 0)\n",
    "    train_f1 = StrListColumnToArray(train_report, \"f1_scores\")\n",
    "    eval_f1 = StrListColumnToArray(eval_report, \"f1_scores\")\n",
    "    train_report = ArrayToColumns(train_report, train_f1, [\"cancer_f1\", \"lat_f1\"])\n",
    "    eval_report = ArrayToColumns(eval_report, eval_f1, [\"cancer_f1\", \"lat_f1\"])\n",
    "    loss_ma = MovingAvg(train_report.loss, window_size)\n",
    "    bf1_ma = MovingAvg(train_report.cancer_f1, window_size)\n",
    "    lr_ma = MovingAvg(train_report.learning_rate, window_size)\n",
    "    tpr_ma = MovingAvg(train_report.tpr, window_size)\n",
    "    tnr_ma = MovingAvg(train_report.tnr, window_size)\n",
    "    train_epoch_stats = train_report.groupby(\"epoch\").mean(numeric_only=True)[[\"learning_rate\", \"tpr\", \"fpr\", \"precision\", \"tnr\", \"cancer_f1\", \"lat_f1\"]]\n",
    "    balf1scores = []\n",
    "    balf1latscores = []\n",
    "    for i in range(len(eval_report.index)):\n",
    "        preds = np.array(ast.literal_eval(eval_report.loc[i, \"predictions\"]))\n",
    "        gt = np.array(ast.literal_eval(eval_report.loc[i, \"truths\"]))\n",
    "        balf1scores.append(BalancedF1Score(preds[:, 0], gt[:, 0]))\n",
    "        balf1latscores.append(BalancedF1Score(preds[:, 1], gt[:, 1]))\n",
    "    lim = len(eval_report.index)\n",
    "    epoch_ticks = eval_report.epoch.astype(np.int16)\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(12,8))\n",
    "    fig.suptitle(\"Training Log\")\n",
    "    # plot loss\n",
    "    axs[0, 0].set_title(\"Loss\")\n",
    "    axs[0, 0].set_xlabel(\"Block\")\n",
    "    axs[0, 0].set_ylabel(\"Loss\")\n",
    "    axs[0, 0].scatter(train_report.index, train_report.loss, s=0.3, c=\"red\", label=\"value\")\n",
    "    axs[0, 0].plot(train_report.index, loss_ma, \"-b\", label=\"moving avg\")\n",
    "    axs[0, 0].legend()\n",
    "    # plot block stats\n",
    "    axs[0, 1].set_title(\"Block Statistics\")\n",
    "    axs[0, 1].set_xlabel(\"Block\")\n",
    "    axs[0, 1].set_ylabel(\"Score\")\n",
    "    axs[0, 1].set_ylim([-0.05, 1.05])\n",
    "    axs[0, 1].scatter(train_report.index, train_report.cancer_f1, s=0.5, c=\"red\", label=\"cancer_f1_score\")\n",
    "    axs[0, 1].scatter(train_report.index, train_report.tpr, marker=\"*\", s=0.5, c=\"blue\", label=\"true_pos_rate\")\n",
    "    axs[0, 1].scatter(train_report.index, train_report.tnr, marker=\"^\", s=0.5, c=\"green\", label=\"true_neg_rate\")\n",
    "    axs[0, 1].plot(train_report.index, bf1_ma, \"-c\")\n",
    "    axs[0, 1].plot(train_report.index, tpr_ma, \"-y\")\n",
    "    axs[0, 1].plot(train_report.index, tnr_ma, \"-m\")\n",
    "    axs[0, 1].legend()\n",
    "    # plot AUC\n",
    "    axs[0, 2].set_title(\"Training AUC\")\n",
    "    axs[0, 2].set_xlabel(\"1 - Specificity (FPR)\")\n",
    "    axs[0, 2].set_xlim([-0.05, 1.05])\n",
    "    axs[0, 2].set_ylim([-0.05, 1.05])\n",
    "    axs[0, 2].set_ylabel(\"Sensitivity (TPR)\")\n",
    "    axs[0, 2].scatter(train_report.fpr, train_report.tpr, s=1., c=\"red\", label=\"train\")\n",
    "    axs[0, 2].scatter(eval_report.fpr, eval_report.tpr, marker=\"*\", s=10.,\n",
    "                      c=\"blue\", label=\"eval\")\n",
    "    axs[0, 2].plot([0., 1.], [0., 1.], \"-k\", linewidth=0.5)\n",
    "    axs[0, 2].legend()\n",
    "    # plot learning rate\n",
    "    axs[1, 0].set_title(\"Learning Rate\")\n",
    "    axs[1, 0].set_xlabel(\"Epoch\")\n",
    "    axs[1, 0].set_ylabel(\"Learning Rate\")\n",
    "    axs[1, 0].plot(train_epoch_stats.index, train_epoch_stats.learning_rate, \"-r\", linewidth=1.)\n",
    "    # axs[1, 0].plot(epoch_ticks, lr_ma, \"-b\")\n",
    "    # plot epoch stats\n",
    "    axs[1, 1].set_title(\"F1 Scores\")\n",
    "    axs[1, 1].set_xlabel(\"Epoch\")\n",
    "    axs[1, 1].set_ylabel(\"Score\")\n",
    "    axs[1, 1].set_ylim([-0.05, 1.05])\n",
    "    axs[1, 1].plot(epoch_ticks, eval_report.cancer_f1, \"-r\",\n",
    "                   linewidth=1.5, label=\"eval_cancer_f1\")\n",
    "    axs[1, 1].plot(epoch_ticks, eval_report.lat_f1, \"-b\",\n",
    "                   linewidth=1.5, label=\"eval_lat_f1\")\n",
    "    axs[1, 1].plot(epoch_ticks, balf1scores, \".r\",\n",
    "                   linewidth=1.5, label=\"bal_cancer_eval_f1\")\n",
    "    axs[1, 1].plot(epoch_ticks, balf1latscores, \".b\",\n",
    "                   linewidth=1.5, label=\"bal_lat_eval_f1\")\n",
    "    axs[1, 1].plot(epoch_ticks, train_epoch_stats.cancer_f1[:lim], \"--r\",\n",
    "                   linewidth=1.5, label=\"train_cancer_f1\")\n",
    "    axs[1, 1].plot(epoch_ticks, train_epoch_stats.lat_f1[:lim], \"--b\",\n",
    "                   linewidth=1.5, label=\"train_lat_f1\")\n",
    "\n",
    "    axs[1, 1].legend()\n",
    "    # plot Recall, Precision and TNR\n",
    "    axs[1, 2].set_title(\"Other Statistics\")\n",
    "    axs[1, 2].set_xlabel(\"Epoch\")\n",
    "    axs[1, 2].set_ylabel(\"Score\")\n",
    "    axs[1, 2].set_ylim([-0.05, 1.05])\n",
    "    axs[1, 2].plot(epoch_ticks, eval_report.tpr, \"-g\",\n",
    "                   linewidth=1.5, label=\"eval_tpr\")\n",
    "    axs[1, 2].plot(epoch_ticks, train_epoch_stats.tpr[:lim], \"--g\",\n",
    "                   linewidth=1., label=\"train_tpr\")\n",
    "    axs[1, 2].plot(epoch_ticks, eval_report.tnr, \"-y\",\n",
    "                   linewidth=1.5, label=\"eval_tnr\")\n",
    "    axs[1, 2].plot(epoch_ticks, train_epoch_stats.tnr[:lim], \"--y\",\n",
    "                   linewidth=1., label=\"train_tnr\")\n",
    "    axs[1, 2].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deff31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_report_path = \"/home/isaiah/kaggle/mammo/results/20230223/densenet_train_rep_02.csv\"\n",
    "val_report_path = \"/home/isaiah/kaggle/mammo/results/20230223/densenet_eval_rep_02.csv\"\n",
    "window_size = 50\n",
    "trep = pd.read_csv(train_report_path)\n",
    "vrep = pd.read_csv(val_report_path)\n",
    "Dashboard2(trep, vrep, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df1db68",
   "metadata": {},
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork().to(\"cpu\")\n",
    "\n",
    "epochs = 100\n",
    "optimizer = Adam(model.parameters(), lr=3.)\n",
    "step_size = 1000\n",
    "scheduler = CyclicLR(optimizer, base_lr=1., max_lr=10., \n",
    "                     step_size_up=step_size, mode=\"triangular2\", \n",
    "                     last_epoch=1000, cycle_momentum=False)\n",
    "for _ in range(step_size):\n",
    "    scheduler.step()   \n",
    "lrs = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    lrs.append(scheduler.get_last_lr())\n",
    "    scheduler.step()\n",
    "fig3 = plt.figure()\n",
    "ax3 = plt.axes()\n",
    "ax3.plot(list(range(1, epochs + 1)), lrs)\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bd4cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
